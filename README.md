# Disaster Response Pipeline Project

This is a project where a NLP and ML Pipeline is bult and also a web app to show the predicted values. We have used two data sets  (disaster_messages.csv and disaster_categories) with messages sent during different disasters and the goal is top classifier those message in 36 different categories in order to speed up the response from the correct entity. The reason is because durin a disaster ( floods, earthquake...etc) the volume of message is huge and it a very big challenge to handle those messages and give the correct action.


### Description

**ETL Pipeline**

    * Load the data from the datasets and combines them 
    * Clean the data
    * Save the cleaned dataset in a SQLite Database called DisasterResponse.db
    
**ML Pipeline**

    * Split data in train and test data
    * Build a pipeline to process the test and create the model
    * Train, tune and show test results
    * Exports the final model as a pickle file
    
**Web App**

    * web app where the model is used to classsify a input text in it's related categories
    * It is used Bootstrap, Plotly and flask

### Instructions:
1. Run the following commands in the project's root directory to set up your database and model.

    - To run ETL pipeline that cleans data and stores in database
        `python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db`
    - To run ML pipeline that trains classifier and saves
        `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`

2. Run the following command in the app's directory to run your web app.
    `python run.py`

3. Go to http://0.0.0.0:3001/


### Files structure :

- app

    - template

        - master.html  # main page of web app

        - go.html  # classification result page of web app

    - run.py  # Flask file that runs app

- data
    - disaster_categories.csv  # data to process
    - disaster_messages.csv  # data to process
    - process_data.py
    - DisasterResponse.db   # database to save clean data to (it is generated by process_data.py)

- models
    - train_classifier.py
    - clean_mode.pkl  # saved model (it is generated by train_classifier.py)

- README.md
